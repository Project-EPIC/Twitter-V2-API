

#===========================================================
# Part 1: 
# What are you looking for?
#===========================================================

# Start and end date of our Query: 
start_time : '2020-01-01T00:00:00Z'    
end_time   : '2020-10-31T00:00:00Z'


# Actual query. See Twitter API Docs for how to build this.
query: 'drought (place:Maine OR place:Vermont OR place:Connecticut OR place:"New Hampshire" OR place:"New York" OR place:"New Jersey" OR place:"Massachusetts" OR place:"Rhode Island")'
    


#===========================================================
# Part 2: 
# What format do you want the data to come back in?
#===========================================================

tweet.fields: 'author_id,created_at,geo,conversation_id,referenced_tweets'


# Add user information
expansions: 'author_id,in_reply_to_user_id,geo.place_id'

user.fields: 'username,description'
place.fields: 'name'



#===========================================================
# Part 3: 
# How do you want your results?

# Important: Do not add additional fields that are not in https://developer.twitter.com/en/docs/twitter-api/tweets/full-archive-search/api-reference/get-tweets-search-all; otherwise will return a 400 Error.
#===========================================================

# Put a limit on the number of requests when pagination is required; The next_token object will be written to disk if limit is reached.
limit: False

# Print the tweets to stdout? You can write tweets to a file easily this way
print: False

# If you put a filename here, tweets will be written to it in line-delimited JSON.
output: 'drought_northeast.jsonseq'


#===========================================================
# Part 4: (OPTIONAL)
# API Call Configuration
#===========================================================

#Tweets per request. Always set to 500 if expecting a lot of data.
max_results : 500

#If you'd like to load the bearer token from here. A token given with --token= will overwrite it.
# bearer_token: AAAA...